{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nsm\n",
    "from nsm import data_utils\n",
    "from nsm import env_factory\n",
    "from nsm import graph_factory\n",
    "from nsm import model_factory\n",
    "from nsm import agent_factory\n",
    "from nsm import executor_factory\n",
    "from nsm import computer_factory\n",
    "from nsm import word_embeddings\n",
    "\n",
    "import experiment as exp\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS  \n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "# Set the level to tf.logging.INFO if you want to see more information.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the path to your data/wikitable folder. \n",
    "# By default it is in ~/projects/data/wikitable. \n",
    "data_dir= os.path.expanduser('~/projects/data/wikitable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.eval_only = True\n",
    "FLAGS.eval_use_gpu = False\n",
    "FLAGS.eval_gpu_id = 0\n",
    "FLAGS.max_n_mem = 60\n",
    "FLAGS.eval_file = os.path.join(data_dir, 'processed_input/preprocess_14/data_split_1/dev_split.jsonl')\n",
    "unittest_file = os.path.join(data_dir, 'processed_input/preprocess_14/data_split_1/train_split_shard_90-0.jsonl')\n",
    "train_file = os.path.join(data_dir, 'processed_input/preprocess_14/train_examples.jsonl')\n",
    "\n",
    "# Fill in the output folder and experiment name you want to load.\n",
    "# By default, load the pretrained model in the repo. \n",
    "FLAGS.output_dir = os.path.expanduser('~/projects/neural-symbolic-machines/table/wtq/')\n",
    "FLAGS.experiment_to_eval = 'pretrained_model'\n",
    "experiment_config = exp.create_experiment_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the agent and the environments in the dev set. \n",
    "# This usually takes 15-30 sec. \n",
    "# If you want to see the training environments, use train_file instead (takes about 75-150 sec). \n",
    "fns = [FLAGS.eval_file]\n",
    "agent, envs = exp.init_experiment(fns, FLAGS.eval_use_gpu, gpu_id=str(FLAGS.eval_gpu_id))\n",
    "for env in envs:\n",
    "    env.punish_extra_work = False\n",
    "env_dict = dict([(env.name, env) for env in envs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on 5 environments / questions and show generated programs. \n",
    "Use the environment id (for example, nt-34) to find the question and its accompanying table in the website below (from Stanford NLP group). \n",
    "https://nlp.stanford.edu/software/sempre/wikitable/viewer/#203-591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the selected 5 environments are 0.8\n",
      "Show the generated programs:\n",
      "\n",
      "env nt-24\n",
      "question: who ranked right after turkey?\n",
      "answer: [u'Sweden', u'Sweden']\n",
      "program: ( filter_str_contain_any all_rows [u'turkey'] r.nation-string ) ( next v12 ) ( hop v13 r.nation-string ) <END>\n",
      "prediction: [u'sweden']\n",
      "return: 1.0\n",
      "prob is 1.0\n",
      "\n",
      "env nt-34\n",
      "question: who was the top ranked competitor in this race?\n",
      "answer: [u'Iryna Shpylova', u'Iryna Shpylova']\n",
      "program: ( first all_rows ) ( hop v7 r.cyclist-string ) <END>\n",
      "prediction: [u'iryna shpylova']\n",
      "return: 1.0\n",
      "prob is 1.0\n",
      "\n",
      "env nt-15\n",
      "question: what was the venue when he placed first?\n",
      "answer: [u'New Delhi, India', u'New Delhi, India']\n",
      "program: ( argmin all_rows r.position-number ) ( first all_rows ) ( diff v10 v9 r.year-number ) <END>\n",
      "prediction: [0.0]\n",
      "return: 0.0\n",
      "prob is 1.0\n",
      "\n",
      "env nt-40\n",
      "question: what was the number of silver medals won by ukraine?\n",
      "answer: [u'2', u'2.0']\n",
      "program: ( filter_str_contain_any all_rows [u'ukraine'] r.nation-string ) ( hop v12 r.silver-number ) <END>\n",
      "prediction: [2.0]\n",
      "return: 1.0\n",
      "prob is 1.0\n",
      "\n",
      "env nt-29\n",
      "question: what is the total population in dzhebariki-khaya?\n",
      "answer: [u'1694', u'1694.0']\n",
      "program: ( filter_str_contain_any all_rows [u'dzhebariki-khaya'] r.urban_settlements-string ) ( hop v13 r.population-number ) <END>\n",
      "prediction: [1694.0]\n",
      "return: 1.0\n",
      "prob is 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 'eval_envs = envs' to evaluate on the whole validation set. Usually takes 8-10 minutes on a laptop. \n",
    "eval_envs = envs[5:10]\n",
    "dev_avg_return, dev_samples, dev_samples_in_beam = exp.beam_search_eval(agent, eval_envs)\n",
    "print('Accuracy on the selected {} environments are {}'.format(len(eval_envs), dev_avg_return))\n",
    "print('Show the generated programs:')\n",
    "print(exp.show_samples(dev_samples, envs[0].de_vocab, env_dict=env_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug the beam search by showing programs in the beam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the 5 programs in beam for environment nt-13901:\n",
      "\n",
      "env nt-13901\n",
      "question: the most points were scored by which player?\n",
      "answer: [u'Karel Hrom\\xe1dka', u'Karel Hrom\\xe1dka']\n",
      "program: ( argmax all_rows r._points-number ) ( hop v10 r.player-string ) <END>\n",
      "prediction: [u'karel hromadka']\n",
      "return: 1.0\n",
      "prob is 0.879124546146\n",
      "\n",
      "env nt-13901\n",
      "question: the most points were scored by which player?\n",
      "answer: [u'Karel Hrom\\xe1dka', u'Karel Hrom\\xe1dka']\n",
      "program: ( argmin all_rows r._points-number ) ( hop v10 r.player-string ) <END>\n",
      "prediction: [u'dawid daniuszewski', u'endre steiner', u'otto zimmermann', u'damian reca', u'giovanni cenni', u'karoly sterk']\n",
      "return: 0.0\n",
      "prob is 0.108504561195\n",
      "\n",
      "env nt-13901\n",
      "question: the most points were scored by which player?\n",
      "answer: [u'Karel Hrom\\xe1dka', u'Karel Hrom\\xe1dka']\n",
      "program: ( maximum all_rows r._points-number ) ( filter_eq all_rows v10 r._points-number ) ( hop v11 r.player-string ) <END>\n",
      "prediction: [u'karel hromadka']\n",
      "return: 1.0\n",
      "prob is 0.00758838433011\n",
      "\n",
      "env nt-13901\n",
      "question: the most points were scored by which player?\n",
      "answer: [u'Karel Hrom\\xe1dka', u'Karel Hrom\\xe1dka']\n",
      "program: ( last all_rows ) ( hop v10 r.player-string ) <END>\n",
      "prediction: [u'etc']\n",
      "return: 0.0\n",
      "prob is 0.00346565319935\n",
      "\n",
      "env nt-13901\n",
      "question: the most points were scored by which player?\n",
      "answer: [u'Karel Hrom\\xe1dka', u'Karel Hrom\\xe1dka']\n",
      "program: ( maximum all_rows r._points-number ) ( filter_not_eq all_rows v10 r._points-number ) ( maximum v11 r._points-number ) <END>\n",
      "prediction: [9.0]\n",
      "return: 0.0\n",
      "prob is 0.0013168551291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_id = 'nt-13901'\n",
    "dev_avg_return, dev_samples, dev_samples_in_beam = exp.beam_search_eval(agent, [env_dict[env_id]])\n",
    "print('Show the {} programs in beam for environment {}:'.format(len(dev_samples_in_beam), env_id))\n",
    "print(exp.show_samples(dev_samples_in_beam, envs[0].de_vocab, env_dict=env_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
